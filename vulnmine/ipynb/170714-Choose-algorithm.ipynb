{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the best classification algorithm\n",
    "\n",
    "### Use a k-fold cross-validation to choose the best classification algorithm\n",
    "\n",
    "From the scikit-learn documentation concerning [k-fold cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "\n",
    ">To avoid it [\"overfitting\"], it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n",
    "\n",
    ">In the basic approach, called *k-fold CV*, the training set is split into k smaller sets... The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "> * A model is trained using k-1 of the folds as training data;\n",
    "* the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The following code uses this technique to evaluate the relative performance of various ML classification algorithms on the training data.\n",
    "\n",
    "RandomForest is one of the best choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'apturl==0.5.2\\nasn1crypto==0.24.0\\natomicwrites==1.1.5\\nattrs==18.1.0\\nbackcall==0.1.0\\nbeautifulsoup4==4.6.0\\nbleach==2.1.3\\nblinker==1.4\\nBrlapi==0.6.4\\ncertifi==2018.4.16\\ncffi==1.11.5\\nchardet==3.0.4\\ncheckbox-support==0.38.0\\ncommand-not-found==0.3\\ncookies==2.2.1\\ncryptography==2.2.2\\ncycler==0.10.0\\ndecorator==4.3.0\\ndefer==1.0.6\\nentrypoints==0.2.3\\nfeedparser==5.2.1\\nfido2==0.3.0\\nfuzzywuzzy==0.16.0\\nguacamole==0.9.2\\nhtml5lib==1.0.1\\nhttplib2==0.11.3\\nidna==2.7\\nipykernel==4.8.2\\nipython==6.4.0\\nipython-genutils==0.2.0\\nipywidgets==7.2.1\\njedi==0.12.1\\nJinja2==2.10\\njsonschema==2.6.0\\njupyter==1.0.0\\njupyter-client==5.2.3\\njupyter-console==5.2.0\\njupyter-core==4.4.0\\nkiwisolver==1.0.1\\nlanguage-selector==0.1\\nlouis==2.6.4\\nlxml==4.2.2\\nMako==1.0.7\\nMarkupSafe==1.0\\nmatplotlib==2.2.2\\nmistune==0.8.3\\nmore-itertools==4.2.0\\nmpmath==1.0.0\\nnbconvert==5.3.1\\nnbformat==4.4.0\\nnotebook==5.5.0\\nnumpy==1.14.5\\noauthlib==2.1.0\\nonboard==1.2.0\\npadme==1.1.1\\npandas==0.23.1\\npandocfilters==1.4.2\\nparso==0.3.0\\npexpect==4.6.0\\npickleshare==0.7.4\\nPillow==5.1.0\\nplainbox==0.38.0\\npluggy==0.6.0\\nprompt-toolkit==1.0.15\\nptyprocess==0.6.0\\npy==1.5.3\\npyasn1==0.4.3\\npycparser==2.18\\npycups==1.9.73\\npycurl==7.43.0\\nPygments==2.2.0\\npygobject==3.20.0\\nPyJWT==1.6.4\\npyOpenSSL==18.0.0\\npyparsing==2.2.0\\npyscard==1.9.2\\npytest==3.6.2\\npython-apt==1.1.0b1+ubuntu0.16.4.1\\npython-dateutil==2.7.3\\npython-debian==0.1.32\\npython-Levenshtein==0.12.0\\npython-systemd==231\\npytz==2018.4\\npyusb==1.0.2\\npyxdg==0.26\\npyzmq==17.0.0\\nqtconsole==4.3.1\\nreportlab==3.4.0\\nrequests==2.19.1\\nrequests-unixsocket==0.1.5\\nresponses==0.9.0\\nschedule==0.5.0\\nscikit-learn==0.19.1\\nscipy==1.1.0\\nSend2Trash==1.5.0\\nsessioninstaller==0.0.0\\nsimplegeneric==0.8.1\\nsix==1.11.0\\nssh-import-id==5.6\\nsympy==1.1.1\\nsystem-service==0.3\\nterminado==0.8.1\\ntestpath==0.3.1\\ntornado==5.0.2\\ntraitlets==4.3.2\\nubuntu-drivers-common==0.0.0\\nufw==0.35\\nunattended-upgrades==0.1\\nunity-scope-calculator==0.1\\nunity-scope-chromiumbookmarks==0.1\\nunity-scope-colourlovers==0.1\\nunity-scope-devhelp==0.1\\nunity-scope-firefoxbookmarks==0.1\\nunity-scope-gdrive==0.7\\nunity-scope-manpages==0.1\\nunity-scope-openclipart==0.1\\nunity-scope-texdoc==0.1\\nunity-scope-tomboy==0.1\\nunity-scope-virtualbox==0.1\\nunity-scope-yelp==0.1\\nunity-scope-zotero==0.1\\nunity-tweak-tool==0.0.7\\nurllib3==1.23\\nusb-creator==0.3.0\\nwcwidth==0.1.7\\nwebencodings==0.5.1\\nwidgetsnbextension==3.2.1\\nxdiagnose==3.8.4.1\\nxkit==0.0.0\\nXlsxWriter==1.0.5\\nxmltodict==0.11.0\\nYapsy==1.11.223\\nyubikey-manager==0.7.0\\n'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import IPython.display\n",
    "\n",
    "# Show versions of all installed software to help debug incompatibilities.\n",
    "# pip runs outside python so use subprocess to get the cmd's O/P.\n",
    "# iPython display is used to print the full O/P.\n",
    "\n",
    "try:\n",
    "    display(subprocess.check_output(['pip', 'freeze']))\n",
    "except subprocess.CalledProcessError as err:\n",
    "    display(err)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the vendor training data\n",
    "\n",
    "Read in the manually labelled vendor training data.\n",
    "\n",
    "Format it and convert to two numpy arrays for input to the scikit-learn ML algorithm.\n",
    "\n",
    "* For Docker, file is _\"/home/jovyan/work/vulnmine/vulnmine_data/label_vendors.csv\"_\n",
    "* For pycharm /iPython, current working directory is *\"~/PycharmProjects/vulnmine\"*. File is *\"~/src/git/vulnmine/vulnmine_data/label_vendors.csv\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10110, 13)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df_label_vendors = pd.io.parsers.read_csv(\n",
    "                            \"~/src/git/vulnmine/vulnmine/vulnmine_data/label_vendors.csv\",\n",
    "                            error_bad_lines=False,\n",
    "                            warn_bad_lines=True,\n",
    "                            quotechar='\"',\n",
    "                            encoding='utf-8')\n",
    "except IOError as e:\n",
    "    display('\\n\\n***I/O error({0}): {1}\\n\\n'.format(\n",
    "                e.errno, e.strerror))\n",
    "\n",
    "# except ValueError:\n",
    "#    self.logger.critical('Could not convert data to an integer.')\n",
    "except:\n",
    "    display(\n",
    "        '\\n\\n***Unexpected error: {0}\\n\\n'.format(\n",
    "            sys.exc_info()[0]))\n",
    "    raise\n",
    "\n",
    "# Number of records / columns\n",
    "\n",
    "df_label_vendors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10110, 7)"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10110,)"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format training data as \"X\" == \"features, \"y\" == target.\n",
    "# The target value is the 1st column.\n",
    "df_match_train1 = df_label_vendors[['match','fz_ptl_ratio', 'fz_ptl_tok_sort_ratio', 'fz_ratio', 'fz_tok_set_ratio', 'fz_uwratio','ven_len', 'pu0_len']]\n",
    "\n",
    "# Convert into 2 numpy arrays for the scikit-learn ML classification algorithms.\n",
    "np_match_train1 = np.asarray(df_match_train1)\n",
    "X, y = np_match_train1[:, 1:], np_match_train1[:, 0]\n",
    "\n",
    "display(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ridge Classifier, Accuracy: 0.97 (+/- 0.02)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Ridge Classifier #2, Accuracy: 0.97 (+/- 0.02)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Perceptron, Accuracy: 0.93 (+/- 0.03)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Passive-Aggressive, Accuracy: 0.87 (+/- 0.16)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'kNN, Accuracy: 0.98 (+/- 0.01)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Nearest Centroid, Accuracy: 0.90 (+/- 0.04)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Random forest, Accuracy: 0.98 (+/- 0.01)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgsec1/PycharmProjects/vulnmine/venv/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n/home/lgsec1/PycharmProjects/vulnmine/venv/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n/home/lgsec1/PycharmProjects/vulnmine/venv/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n/home/lgsec1/PycharmProjects/vulnmine/venv/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n/home/lgsec1/PycharmProjects/vulnmine/venv/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SGD / SVM, Accuracy: 0.93 (+/- 0.02)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Naive Bayes, Accuracy: 0.78 (+/- 0.15)'"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up for k-fold cross-validation to choose best model\n",
    "\n",
    "#rom sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "for clf, clf_name in (\n",
    "\t\t(RidgeClassifier(alpha=1.0), \"Ridge Classifier\"),\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier #2\"),\n",
    "        (Perceptron(max_iter=100),\"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(max_iter=100),\"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (NearestCentroid(), \"Nearest Centroid\"),\n",
    "        (RandomForestClassifier(n_estimators=100, class_weight=\"balanced\"), \"Random forest\"),\n",
    "\t\t(SGDClassifier(alpha=.0001, penalty=\"l2\"), \"SGD / SVM\"),\n",
    "\t\t(MultinomialNB(alpha=.01), \"Naive Bayes\")):\n",
    "\n",
    "\tscores = cross_val_score(clf, X, y, cv=5)\n",
    "\tdisplay(\"%s, Accuracy: %0.2f (+/- %0.2f)\" % (clf_name, scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
